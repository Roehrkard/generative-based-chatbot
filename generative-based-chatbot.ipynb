{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "054b6f00",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "054b6f00",
        "outputId": "b19055f7-0f93-4d17-de7b-64f55b5399a7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.26.4)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.2.2)\n",
            "Requirement already satisfied: numpy>=1.22.4 in /usr/local/lib/python3.10/dist-packages (from pandas) (1.26.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.5.2)\n",
            "Requirement already satisfied: numpy>=1.19.5 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.26.4)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.13.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.5.0)\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.10/dist-packages (2.17.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (24.3.25)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: h5py>=3.10.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.11.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: ml-dtypes<0.5.0,>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.4.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow) (24.1)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.20.3)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.32.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow) (71.0.4)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.16.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.5.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (4.12.2)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.16.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.64.1)\n",
            "Requirement already satisfied: tensorboard<2.18,>=2.17 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.17.0)\n",
            "Requirement already satisfied: keras>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.4.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.37.1)\n",
            "Requirement already satisfied: numpy<2.0.0,>=1.23.5 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.26.4)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow) (0.44.0)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->tensorflow) (13.9.2)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->tensorflow) (0.0.8)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->tensorflow) (0.13.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (2024.8.30)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow) (3.7)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow) (3.0.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.18,>=2.17->tensorflow) (2.1.5)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras>=3.2.0->tensorflow) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras>=3.2.0->tensorflow) (2.18.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.2.0->tensorflow) (0.1.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install numpy\n",
        "!pip install pandas\n",
        "!pip install scikit-learn\n",
        "!pip install tensorflow"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Access dataset\n",
        "file_path = '/content/drive/My Drive/colab_datasets/dialogueText_301.csv'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SlGcSQnspTQ2",
        "outputId": "c35b8741-2bdf-42b3-c739-913556d7e119"
      },
      "id": "SlGcSQnspTQ2",
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f245c30d",
      "metadata": {
        "id": "f245c30d"
      },
      "source": [
        "## 1. Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "af5953eb",
      "metadata": {
        "id": "af5953eb"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dropout, Dense\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.mixed_precision import set_global_policy\n",
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "716de337",
      "metadata": {
        "id": "716de337"
      },
      "source": [
        "## 2. Function Definitions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "17963403",
      "metadata": {
        "id": "17963403"
      },
      "outputs": [],
      "source": [
        "# Set mixed precision policy\n",
        "set_global_policy('mixed_float16')\n",
        "\n",
        "def load_data(file_path):\n",
        "    \"\"\"Load data from a CSV file and select relevant columns.\"\"\"\n",
        "    data = pd.read_csv(file_path)\n",
        "    data = data[['from', 'to', 'text']]  # Select relevant columns\n",
        "    return data\n",
        "\n",
        "def preprocess_text(data, max_len=50):\n",
        "    \"\"\"Preprocess text data by tokenizing and padding sequences.\"\"\"\n",
        "    data = data.dropna(subset=['text'])  # Drop rows with missing text\n",
        "\n",
        "    # Tokenizer initialization and fitting\n",
        "    tokenizer = Tokenizer()\n",
        "    tokenizer.fit_on_texts(data['text'].astype(str))  # Fit tokenizer on text\n",
        "\n",
        "    # Convert texts to sequences and pad them\n",
        "    sequences = tokenizer.texts_to_sequences(data['text'].astype(str))\n",
        "    padded_sequences = pad_sequences(sequences, maxlen=max_len, padding='post')\n",
        "\n",
        "    return padded_sequences, tokenizer\n",
        "\n",
        "def build_model(vocab_size, max_len):\n",
        "    model = Sequential()\n",
        "    model.add(Embedding(input_dim=vocab_size, output_dim=128, input_length=max_len))  # Embedding layer\n",
        "    model.add(LSTM(64, return_sequences=True))  # First LSTM layer, return sequences for each timestep\n",
        "    model.add(Dropout(0.2))\n",
        "    model.add(LSTM(64, return_sequences=True))  # Second LSTM layer, return sequences for each timestep\n",
        "    model.add(Dense(vocab_size, activation='softmax', dtype='float32'))  # Predict a word (softmax) for each timestep\n",
        "    return model"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3793e0a7",
      "metadata": {
        "id": "3793e0a7"
      },
      "source": [
        "## 3. Model Selection"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "33af55b9",
      "metadata": {
        "id": "33af55b9"
      },
      "outputs": [],
      "source": [
        "# Load and preprocess data\n",
        "data = load_data(file_path)\n",
        "padded_sequences, tokenizer = preprocess_text(data)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "384a5513",
      "metadata": {
        "id": "384a5513"
      },
      "source": [
        "## 4. Training Model (Train/Validate/Test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a488024e",
      "metadata": {
        "id": "a488024e"
      },
      "outputs": [],
      "source": [
        "# Prepare input/output data\n",
        "X, y = padded_sequences[:, :-1], padded_sequences[:, 1:]\n",
        "\n",
        "# Split into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# No reshaping needed; target remains 2D\n",
        "y_train = y_train\n",
        "y_test = y_test\n",
        "\n",
        "# Convert data to tf.data.Dataset for better performance\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices((X_train, y_train)).batch(128).prefetch(tf.data.AUTOTUNE)\n",
        "test_dataset = tf.data.Dataset.from_tensor_slices((X_test, y_test)).batch(128).prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "# Build model\n",
        "vocab_size = len(tokenizer.word_index) + 1  # Account for padding\n",
        "max_len = padded_sequences.shape[1]\n",
        "\n",
        "model = build_model(vocab_size, max_len)\n",
        "\n",
        "# Compile model with optimizer tweaks\n",
        "optimizer = Adam(learning_rate=1e-3)  # Initial learning rate\n",
        "model.compile(optimizer=optimizer, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Add callbacks for checkpointing, early stopping, and learning rate adjustment\n",
        "checkpoint = ModelCheckpoint('best_model.keras', monitor='val_loss', save_best_only=True)\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=2, restore_best_weights=True)\n",
        "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=1, min_lr=1e-6)  # Reduces LR on plateau\n",
        "\n",
        "# Train model with callbacks\n",
        "history = model.fit(train_dataset, validation_data=test_dataset, epochs=10, batch_size=16, callbacks=[checkpoint, early_stopping, reduce_lr])\n",
        "\n",
        "# Evaluate model performance on test data\n",
        "results = model.evaluate(test_dataset)\n",
        "print(f\"Test Loss: {results[0]}\")\n",
        "print(f\"Test Accuracy: {results[1]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "31ccbf5f",
      "metadata": {
        "id": "31ccbf5f"
      },
      "source": [
        "## 5. Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a989ed43",
      "metadata": {
        "id": "a989ed43"
      },
      "outputs": [],
      "source": [
        "# Evaluate model\n",
        "results = model.evaluate(test_dataset)\n",
        "print(f\"Test Loss: {results[0]}\")\n",
        "print(f\"Test Accuracy: {results[1]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3fdb8a36",
      "metadata": {
        "id": "3fdb8a36"
      },
      "source": [
        "## 6. Hyperparameter Tuning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "72036a13",
      "metadata": {
        "id": "72036a13"
      },
      "outputs": [],
      "source": [
        "# Hyperparameter tuning example (adjusting LSTM units and dropout rates)\n",
        "def hyperparameter_tuning():\n",
        "    best_accuracy = 0\n",
        "    for units in [64, 128]:  # Exploring different LSTM units\n",
        "        for dropout in [0.2, 0.3]:  # Exploring different dropout rates\n",
        "            model = Sequential([\n",
        "                Embedding(input_dim=vocab_size, output_dim=128),\n",
        "                LSTM(units, return_sequences=True),\n",
        "                Dropout(dropout),\n",
        "                LSTM(units),\n",
        "                Dense(vocab_size, activation='softmax')\n",
        "            ])\n",
        "            model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "            history = model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=5, batch_size=64)\n",
        "\n",
        "            # Extract and compare validation accuracy\n",
        "            accuracy = history.history['val_accuracy'][-1]\n",
        "            if accuracy > best_accuracy:\n",
        "                best_accuracy = accuracy\n",
        "    return best_accuracy"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e4fedf5a",
      "metadata": {
        "id": "e4fedf5a"
      },
      "source": [
        "## 7. Evaluate and Compare Results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "16c1c2a8",
      "metadata": {
        "id": "16c1c2a8"
      },
      "outputs": [],
      "source": [
        "# Compare results after tuning hyperparameters\n",
        "best_accuracy = hyperparameter_tuning()\n",
        "print(f\"Best Validation Accuracy after tuning: {best_accuracy}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "be7f9e7c",
      "metadata": {
        "id": "be7f9e7c"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python (myenv)",
      "language": "python",
      "name": "myenv"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    },
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}